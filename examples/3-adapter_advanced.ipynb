{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapters in bigger pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we show how to use adapters to create more complicated pipelines in Steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "import traceback\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.base import Step, BaseTransformer, Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate the pipeline for digits recognition from notebook #1.\n",
    "\n",
    "We start off by fetching the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = './cache'\n",
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, test_size=0.2, stratify=y_digits, random_state=42)\n",
    "\n",
    "print('{} samples for training'.format(len(y_train)))\n",
    "print('{} samples for test'.format(len(y_test)))\n",
    "\n",
    "data_train = {'input':\n",
    "                {\n",
    "                     'images': X_train,\n",
    "                     'labels': y_train,\n",
    "                }\n",
    "            }\n",
    "\n",
    "data_test = {'input':\n",
    "                {\n",
    "                     'images': X_test,\n",
    "                     'labels': y_test,\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `RandomForestTransformer` in similar manner as before. With one difference, though. `Transform` will use RandomForest's `predict_proba` instead of `predict` which will be useful in the latter part of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestTransformer(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        self.estimator = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=12345)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        y_proba  = self.estimator.predict_proba(X)\n",
    "        return {'y_proba': y_proba}\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        joblib.dump(self.estimator, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        self.estimator = joblib.load(filepath)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_step = Step(name='random_forest',\n",
    "               transformer=RandomForestTransformer(),\n",
    "               input_data=['input'],        \n",
    "               cache_dirpath=CACHE_DIR,\n",
    "               force_fitting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph looks just like in notebook #1. Let's try to execute it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    preds_train_rf = rf_step.fit_transform(data_train)\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, something went wrong. The problem is that `input` dictionary in `data_train` contains fields `images` and `labels`, whereas `RandomForestTransformer` expects arguments `X` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The solution: adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle such issues, `Step`'s initializer has `adapter` argument. `Adapter` describes how to reshape the data from the input nodes into the form expected by the transformer or further steps. \n",
    "\n",
    "The basic usage is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_step = Step(name='rename',\n",
    "               transformer=Dummy(),\n",
    "               input_data=['input'],\n",
    "               adapter={'X': [('input', 'images')],\n",
    "                        'y': [('input', 'labels')]},\n",
    "               cache_dirpath=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a new step which gets its data from `input` node.\n",
    "\n",
    "When the program flow gets to `rename_step`, first `adapter`-related code is executed. `RandomForestTransformer`'s `fit_transform` and `transform` methods expect arguments `X` and `y`. The `adapter` is basically a dictionary which for each expected argument tells how to get it. For instance `'X': [('input', 'images')]` tells the step, that value for `X` is stored under `images` key in the dictionary returned by `input` node.\n",
    "\n",
    "Transformer inside this step is `Dummy` which means that its result is a dictionary described by the adapter.\n",
    "\n",
    "Let's try to fit Random Forest again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_step = Step(name='random_forest',\n",
    "               transformer=RandomForestTransformer(),\n",
    "               input_steps=[rename_step],        \n",
    "               cache_dirpath=CACHE_DIR,\n",
    "               force_fitting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_step.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it worked like charm - we see class probabilites for the train cases.\n",
    "\n",
    "Note, that instead of creating a new step `rename`, we could have put `adapter` in `RandomForestTransformer` like this:\n",
    "```\n",
    "rf_step = Step(name='random_forest',\n",
    "               transformer=RandomForestTransformer(),\n",
    "               input_data=['input'],\n",
    "               adapter={'X': [('input', 'images')],\n",
    "                        'y': [('input', 'labels')]},     \n",
    "               cache_dirpath=CACHE_DIR)\n",
    "```\n",
    "and the result would be exactly the same. However, this renaming will be necessary in further steps, so we decided to go with a proxy step. Otherwise, we would have to copy this adapter in all other steps that expect `X` and `y` instead of `images` and `labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline with model ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often when we have multiple models which perform on the same level it makes sense to combine them. The created model ensembling tends to be more stable and can even improve results a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's train another model! This time we will use XGBoost. What we need to do is really analogous to what we did for Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostTransformer(BaseTransformer):\n",
    "    def __init__(self, xgb_params, num_boost_round):\n",
    "        self.estimator = None\n",
    "        self.xgb_params = xgb_params\n",
    "        self.num_boost_round = num_boost_round\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tr_mat = xgboost.DMatrix(X, label=y)\n",
    "        evals = [(tr_mat, 'train')]\n",
    "        self.estimator = xgboost.train(self.xgb_params,\n",
    "                                       tr_mat,\n",
    "                                       num_boost_round=self.num_boost_round,\n",
    "                                       verbose_eval=False,\n",
    "                                       evals=evals)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        test_mat = xgboost.DMatrix(X)\n",
    "        y_proba  = self.estimator.predict(test_mat)\n",
    "        return {'y_proba': y_proba}\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        joblib.dump({'estimator': self.estimator,\n",
    "                     'xgb_params': self.xgb_params,\n",
    "                     'num_boost_round': self.num_boost_round},\n",
    "                    filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        d = joblib.load(filepath)\n",
    "        self.estimator = d['estimator']\n",
    "        self.xgb_params = d['xgb_params']\n",
    "        self.num_boost_round = d['num_boost_round']\n",
    "        return self\n",
    "    \n",
    "def get_xgb_params():\n",
    "    return {\n",
    "        'objective': 'multi:softprob',\n",
    "        \"num_class\": 10,\n",
    "        'eta': 0.5,\n",
    "        'max_depth': 4,\n",
    "        'silent': True,\n",
    "        'nthread': -1,\n",
    "        'lambda': 1.0,\n",
    "        'eval_metric': [\"mlogloss\", \"merror\"]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_step = Step(name='xgboost',\n",
    "               transformer=XGBoostTransformer(xgb_params=get_xgb_params(), num_boost_round=5),\n",
    "               input_steps=[rename_step],\n",
    "               cache_dirpath=CACHE_DIR,\n",
    "               force_fitting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_step.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, so now we have two models trained, but we haven't checked so far how do they perform. Let's do it now. We will use one of data scientists' favourite measures: log-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_proba_train = rf_step.transform(data_train)['y_proba']\n",
    "rf_proba_test = rf_step.transform(data_test)['y_proba']\n",
    "xgb_proba_train = xgb_step.transform(data_train)['y_proba']\n",
    "xgb_proba_test = xgb_step.transform(data_test)['y_proba']\n",
    "\n",
    "print(\"RF train: {:.3f}, test: {:.3f}\".format(log_loss(y_pred=rf_proba_train, y_true=y_train),\n",
    "                                      log_loss(y_pred=rf_proba_test, y_true=y_test)))\n",
    "print(\"XGB train: {:.3f}, test: {:.3f}\".format(log_loss(y_pred=xgb_proba_train, y_true=y_train),\n",
    "                                       log_loss(y_pred=xgb_proba_test, y_true=y_test)))\n",
    "print(\"Averaged predictions: {:.3f}, test: {:.3f}\".format(log_loss(y_pred=(rf_proba_train + xgb_proba_train) / 2, y_true=y_train),\n",
    "                                            log_loss(y_pred=(rf_proba_test + xgb_proba_test) / 2, y_true=y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we noticeably overfit, but it's OK - making a perfect model is not the goal of this notebook. We also see that we could benefit from a very simple model ensembling: averaging of model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling and evaluation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step incorporates averaging of model predictions into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_step = Step(name='ensembler',\n",
    "                 transformer=Dummy(),\n",
    "                 input_steps=[rf_step, xgb_step],                 \n",
    "                 adapter={'y_proba': ([(rf_step.name, 'y_proba'),\n",
    "                                      (xgb_step.name, 'y_proba')],\n",
    "                                     lambda lst: np.array(lst).mean(axis=0))\n",
    "                         },\n",
    "                cache_dirpath=CACHE_DIR,\n",
    "                force_fitting=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a little different syntax in `adapter` this time. Recipe for `y_proba` consists of two things:\n",
    "- a list of objects returned by input steps that should be used to build `y_proba`,\n",
    "- a function which merges them into a final `y_proba` object.\n",
    "\n",
    "So `[(rf_step.name, 'y_proba'), (xgb_step.name, 'y_proba')]` tells the adapter to extract `y_proba` arrays from dictionaries returned by `rf_step` and `xgb_step` and put them in a list. Then `lambda lst: np.array(lst).mean(axis=0)` will average these arrays.\n",
    "\n",
    "An adapter is actually a description of how to build arguments for `fit_transform` and `transform`. It is a dictionary, where:\n",
    "- keys must agree with transormer's `fit_transform` and `transform` arguments,\n",
    "- values must be either:\n",
    "  1. a brick description,\n",
    "  2. a pair of:\n",
    "    - a list of brick descriptions,\n",
    "    - a function that merges extracted results of previous steps,\n",
    "\n",
    "where _brick description_ is a pair of node name and key in the dictionary returned by that node.\n",
    "\n",
    "Step with an adapter proceeds like this:\n",
    "1. It gathers results from preceeding nodes.\n",
    "2. It builds a dictionary with the same keys as the adapter and with values built according to descriptions:\n",
    "   - if the key in the adapter maps to a single brick description, an appropriate object is extracted from the results of input nodes,\n",
    "   - in the other case, objects are extracted according to brick descriptions and added to a list, which is then passed to a function that generates final object.\n",
    "3. Arguments of `fit_transform` and `transform` are filled using the above dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our ensembling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_step.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fine! However, often we are interested only in the class with the highest probability. Let's make a step that will find this class for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses_step = Step(name='guesses_maker',\n",
    "                 transformer=Dummy(),\n",
    "                 input_steps=[ensemble_step],                 \n",
    "                 adapter={'y_pred': ([(ensemble_step.name, 'y_proba')],\n",
    "                                     lambda lst: np.argmax(lst[0], axis=1))\n",
    "                         },\n",
    "                 cache_dirpath=CACHE_DIR,\n",
    "                 force_fitting=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be already familiar with everything that happened here. New step, `guesses_maker`, takes its input from `ensembler`. Adapter will create just one element: `y_pred`. List of bricks used to build `y_pred` has only one element:  `y_proba` found in `ensembler`'s result. Function `lambda lst: np.argmax(lst[0], axis=1)` takes this list and performs row-wise `argmax` on its only element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses_step.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one of the previous cells we checked quality of our model manually. Let's add a final step that will do it for us automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationTransformer(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, y_true, y_proba, y_pred):\n",
    "        return self\n",
    "\n",
    "    def transform(self, y_true, y_proba, y_pred):\n",
    "        #print(len(y_true), len(y_pred))\n",
    "        #print(y_proba)\n",
    "        return {'Log-loss': log_loss(y_pred=y_proba, y_true=y_true),\n",
    "                'Acc:': '{:.2f}'.format(sum(y_true == y_pred) / len(y_pred))\n",
    "               }\n",
    "\n",
    "evaluation_step = Step(name='evaluator',\n",
    "                 transformer=EvaluationTransformer(),\n",
    "                 input_steps=[ensemble_step, guesses_step, rename_step],                 \n",
    "                 adapter={'y_proba': [(ensemble_step.name, 'y_proba')],\n",
    "                          'y_pred':  [(guesses_step.name, 'y_pred')],\n",
    "                          'y_true': [(rename_step.name, 'y')]\n",
    "                         },\n",
    "                 cache_dirpath=CACHE_DIR\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_step.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluation_step.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek on pipeline predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always very pleasant to compare images with model's predictions. As a last example we show a step that displays a few images with the predicted probability distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeekOnPredictions(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, ens_proba, rf_proba, xgb_proba, images):\n",
    "        return self\n",
    "\n",
    "    def transform(self, ens_proba, rf_proba, xgb_proba, images): \n",
    "        pd.options.display.float_format = '{:6.3f}'.format\n",
    "        for i in range(5):\n",
    "            df = pd.DataFrame({'rf': rf_proba[i], 'xgb': xgb_proba[i], 'ens': ens_proba[i]}, index=list(range(10)))\n",
    "            plt.figure(figsize=(6,2))\n",
    "            left =  plt.subplot(1, 2, 1)\n",
    "            right = plt.subplot(1, 2, 2)\n",
    "            left.imshow(images[i].reshape(8, 8), cmap='gray')\n",
    "            right.axis('off')\n",
    "            right.text(0, 0.3, str(df.T), fontsize=16, fontname='monospace')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peek_step = Step(name='peek',\n",
    "                 transformer=PeekOnPredictions(),\n",
    "                 input_steps=[ensemble_step, rf_step, xgb_step],\n",
    "                 input_data=['input'],\n",
    "                 adapter={'ens_proba': [(ensemble_step.name, 'y_proba')],\n",
    "                          'rf_proba':  [(rf_step.name, 'y_proba')],\n",
    "                          'xgb_proba': [(xgb_step.name, 'y_proba')],\n",
    "                          'images': [('input', 'images')]\n",
    "                         },\n",
    "                 cache_dirpath=CACHE_DIR\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peek_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peek_step.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peek_step.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
